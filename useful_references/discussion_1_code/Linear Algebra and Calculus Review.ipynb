{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CSE 151B: Linear Algebra and Calculus Review"],"metadata":{"id":"NNP3MACYsEHD"}},{"cell_type":"code","source":["# import PyTorch\n","import torch"],"metadata":{"id":"6-9rm12_sNfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Linear Algebra"],"metadata":{"id":"2JtfKsnhsKMb"}},{"cell_type":"markdown","source":["### Scalars, Vectors, Matrices, and Tensors"],"metadata":{"id":"sMEghyFBsVW1"}},{"cell_type":"markdown","source":["A Scalar $\\mathbf{x} \\in \\mathbb{R}$ is a real number.\n","\n","A vector $\\mathbf{y} \\in \\mathbb{R}^n = \\begin{pmatrix}\\mathbf{y}_1\\\\\\mathbf{y}_2\\\\\\vdots\\\\\\mathbf{y}_n\\end{pmatrix}$ is a fixed-length array of scalars.\n","\n","A matrix $\\mathbf{z} \\in \\mathbb{R}^{m \\times n} = \\begin{pmatrix}\\mathbf{z}_{11} && \\mathbf{z}_{12} && \\dots && \\mathbf{z}_{1n}\\\\\\mathbf{z}_{21} && \\mathbf{z}_{22} && \\dots && \\mathbf{z}_{2n}\\\\\\vdots && \\vdots && \\ddots && \\vdots\\\\\\mathbf{z}_{m1} && \\mathbf{z}_{m2} && \\dots && \\mathbf{z}_{mn}\\end{pmatrix}$ is a second-order tensor (or an array or arrays if you will) consisting of two axes---rows and columns.\n","\n","A tensor extends this idea into higher dimensions; it is an $n^{th}$-order array.\n","\n","For example,  \n","- A single grayscale image (having a single channel) is represented as a second-order tensor: $\\mathbf{H} \\times \\mathbf{W}$\n","- A single colored image (having 3 channels RGB) is represented as a third-order tensor: $3 \\times \\mathbf{H} \\times \\mathbf{W}$\n","- A batch (collection) of grayscale images is a third-order tensor: $\\mathbf{N} \\times \\mathbf{H} \\times \\mathbf{W}$\n","- A batch (collection) of colored images is a fourth-order tensor: $\\mathbf{N} \\times 3 \\times \\mathbf{H} \\times \\mathbf{W}$\n"],"metadata":{"id":"E784fd4asYH6"}},{"cell_type":"code","source":["x = torch.tensor(3.0)\n","print(\"A scalar x:\", x)\n","\n","y = torch.arange(5)\n","print(\"\\nA vector y:\", y)\n","print(\"Length of vector y:\", len(y))\n","print(\"2nd element of vector y:\", y[1])  # zero-based indexing\n","\n","z = torch.arange(6).reshape(3, 2)\n","print(\"\\nA matrix z:\\n\", z)\n","print(\"Number of rows:\", z.shape[0])\n","print(\"Number of columns:\", z.shape[1])\n","print(\"Element at position (2,1):\", z[2][1])\n","print(\"Transpose of matrix z:\\n\", z.T)\n","\n","w = torch.arange(12).reshape(2,2,3)\n","print(\"\\nA 3rd-order tensor w:\\n\", w)\n","print(\"Shape of w:\", w.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1bA-X9JsJUa","executionInfo":{"status":"ok","timestamp":1680664838874,"user_tz":420,"elapsed":296,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"2b1b5a2e-46f4-4ea1-d200-b7d091bcfd16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A scalar x: tensor(3.)\n","\n","A vector y: tensor([0, 1, 2, 3, 4])\n","Length of vector y: 5\n","2nd element of vector y: tensor(1)\n","\n","A matrix z:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n","Number of rows: 3\n","Number of columns: 2\n","Element at position (2,1): tensor(5)\n","Transpose of matrix z:\n"," tensor([[0, 2, 4],\n","        [1, 3, 5]])\n","\n","A 3rd-order tensor w:\n"," tensor([[[ 0,  1,  2],\n","         [ 3,  4,  5]],\n","\n","        [[ 6,  7,  8],\n","         [ 9, 10, 11]]])\n","Shape of w: torch.Size([2, 2, 3])\n"]}]},{"cell_type":"markdown","source":["### Elementwise Operations"],"metadata":{"id":"6OlvpQ1Gtx5K"}},{"cell_type":"code","source":["a = torch.arange(6).reshape(3,2)\n","b = a.clone()\n","print(\"a:\\n\", a)\n","print(\"b:\\n\", b)\n","print(\"Elementwise Addition:\\n\", a + b)\n","print(\"Elementwise Multiplication:\\n\", a * b)\n","print(\"Elementwise Function Application:\\n\", torch.square(a))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35DHaGHxttZX","executionInfo":{"status":"ok","timestamp":1680664839214,"user_tz":420,"elapsed":342,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"148c7af3-ea7a-43c5-84b5-4159fe4f37e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n","b:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n","Elementwise Addition:\n"," tensor([[ 0,  2],\n","        [ 4,  6],\n","        [ 8, 10]])\n","Elementwise Multiplication:\n"," tensor([[ 0,  1],\n","        [ 4,  9],\n","        [16, 25]])\n","Elementwise Function Application:\n"," tensor([[ 0,  1],\n","        [ 4,  9],\n","        [16, 25]])\n"]}]},{"cell_type":"markdown","source":["### Reduction\n","\n","For a matrix $\\mathbf{z} \\in \\mathbb{R}^{m \\times n} = \\begin{pmatrix}\\mathbf{z}_{11} && \\mathbf{z}_{12} && \\dots && \\mathbf{z}_{1n}\\\\\\mathbf{z}_{21} && \\mathbf{z}_{22} && \\dots && \\mathbf{z}_{2n}\\\\\\vdots && \\vdots && \\ddots && \\vdots\\\\\\mathbf{z}_{m1} && \\mathbf{z}_{m2} && \\dots && \\mathbf{z}_{mn}\\end{pmatrix}$,"],"metadata":{"id":"2E4Qusv51V-w"}},{"cell_type":"code","source":["z = torch.arange(10).reshape(5,2)\n","print(\"z:\\n\", z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAL3K1Q72Nub","executionInfo":{"status":"ok","timestamp":1680664839214,"user_tz":420,"elapsed":55,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"f76ef724-f2d1-4e7a-a304-0328398a8893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["z:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])\n"]}]},{"cell_type":"markdown","source":["[A] $\\sum_{i=0}^m \\sum_{j=0}^n \\mathbf{z}[i][j]$"],"metadata":{"id":"TgxXdwv11f4x"}},{"cell_type":"code","source":["print(torch.sum(z))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IABXPINP1VYj","executionInfo":{"status":"ok","timestamp":1680664839214,"user_tz":420,"elapsed":42,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"462f9f91-4291-4e21-889e-b1bd98e55359"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(45)\n"]}]},{"cell_type":"markdown","source":["[B] $\\sum_{i=0}^m \\mathbf{z}[i][j]$  $\\forall j \\in \\{0, \\dots, n\\}$"],"metadata":{"id":"ExtTRESF3SwD"}},{"cell_type":"code","source":["print(torch.sum(z, axis=0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPd5qilB3d8Q","executionInfo":{"status":"ok","timestamp":1680664839215,"user_tz":420,"elapsed":33,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"e08d4ec9-3d01-46a0-e805-fae76402d553"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([20, 25])\n"]}]},{"cell_type":"markdown","source":["[C] $\\sum_{j=0}^n \\mathbf{z}[i][j]$  $\\forall i \\in \\{0, \\dots, m\\}$"],"metadata":{"id":"rQKRLh-b2hJW"}},{"cell_type":"code","source":["print(torch.sum(z, axis=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mg9T0F4N0rZp","executionInfo":{"status":"ok","timestamp":1680664839215,"user_tz":420,"elapsed":21,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"0f572039-33c5-4806-d763-84466738a2e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1,  5,  9, 13, 17])\n"]}]},{"cell_type":"markdown","source":["### Non-Reduction\n","\n","Notice how, in the above examples, the result of `torch.sum` does not maintain the original shape of the tensor. Everything that is output is converted into a single-axis vector.\n","\n","If we want to preserve the axes, pass the `keepdim=True` argument."],"metadata":{"id":"MbzvwEuX3jmD"}},{"cell_type":"code","source":["print(torch.sum(z, axis=0, keepdim=True))  # should maintain the 2 columns\n","print(torch.sum(z, axis=1, keepdim=True))  # should maintain the 5 rows"],"metadata":{"id":"7bcJxrW52r1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680664839785,"user_tz":420,"elapsed":582,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"9caf104f-73b4-4679-bc98-c2b2a7551983"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[20, 25]])\n","tensor([[ 1],\n","        [ 5],\n","        [ 9],\n","        [13],\n","        [17]])\n"]}]},{"cell_type":"markdown","source":["The relevance of `keepdim=True` comes into play when you want to broadcast a tensor.\n","\n","Consider performing the mean (or average) operation along every row of a matrix:"],"metadata":{"id":"_5srHRCh4VXV"}},{"cell_type":"code","source":["z = torch.arange(10).reshape(5,2)\n","print(\"z:\\n\", z)\n","\n","sum = torch.sum(z, axis=1)\n","print(\"sum along every row:\", sum)\n","\n","print(\"mean along every row:\\n\", z / sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"uqxVtlea4DAm","executionInfo":{"status":"error","timestamp":1680664840495,"user_tz":420,"elapsed":712,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"35613242-007c-4bcb-fd4b-917729f60cf8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["z:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])\n","sum along every row: tensor([ 1,  5,  9, 13, 17])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9fe7f6698a0f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum along every row:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean along every row:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1"]}]},{"cell_type":"markdown","source":["Now, use `keepdim=True` to preserve axes:"],"metadata":{"id":"Cq4txOzg5FSg"}},{"cell_type":"code","source":["z = torch.arange(10).reshape(5,2)\n","print(\"z:\\n\", z)\n","\n","sum = torch.sum(z, axis=1, keepdim=True)\n","print(\"sum along every row:\\n\", sum)\n","\n","print(\"mean along every row:\\n\", z / sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MOuOR3P5Esr","executionInfo":{"status":"ok","timestamp":1680664846072,"user_tz":420,"elapsed":416,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"9a81a430-fcda-4a4a-ef41-940f126d0a81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["z:\n"," tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])\n","sum along every row:\n"," tensor([[ 1],\n","        [ 5],\n","        [ 9],\n","        [13],\n","        [17]])\n","mean along every row:\n"," tensor([[0.0000, 1.0000],\n","        [0.4000, 0.6000],\n","        [0.4444, 0.5556],\n","        [0.4615, 0.5385],\n","        [0.4706, 0.5294]])\n"]}]},{"cell_type":"markdown","source":["Voila! What is happening behind the scenes is:\n","\n","$\\mathbf{z} = \\begin{pmatrix}0 && 1\\\\2 && 3\\\\4&&5\\\\6 && 7\\\\8 && 9\\end{pmatrix}$\n","\n","$\\text{sum} = \\begin{pmatrix}1\\\\5\\\\9\\\\13\\\\17\\end{pmatrix}$\n","\n","`sum` is broadcasted to match the dimensions of `z`:\n","\n","$\\text{broadcasted sum} = \\begin{pmatrix}1 && 1\\\\5 && 5\\\\9 && 9\\\\13 && 13\\\\17 && 17\\end{pmatrix}$\n","\n","And finally,\n","\n","$\\mathbf{z} / \\text{broadcasted sum} = \\begin{pmatrix}0/1 && 1/1\\\\2/5 && 3/5\\\\4/9 && 5/9\\\\6/13 && 7/13\\\\8/17 && 9/17\\end{pmatrix} = \\begin{pmatrix}0 && 1\\\\0.4 && 0.6\\\\0.4444 && 0.5556\\\\0.4615 && 0.5385\\\\0.4706 && 0.5294\\end{pmatrix}$"],"metadata":{"id":"7byz8tFu5dRN"}},{"cell_type":"markdown","source":["### Dot Product, Matrix-Vector Product, and Matrix-Matrix Product"],"metadata":{"id":"RCfRnjN97JMJ"}},{"cell_type":"markdown","source":["Dot product over two vectors $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^d$ is $\\mathbf{x}^T\\mathbf{y} = \\mathbf{y}^T\\mathbf{x}=\\langle \\mathbf{x},\\mathbf{y} \\rangle = \\sum_{i=0}^d \\mathbf{x}_i\\mathbf{y}_i$\n","\n","A Matrix-Vector product $\\mathbf{A}\\mathbf{x}$ is a matrix comprising as rows the dot product of each row of $\\mathbf{A}$ with $\\mathbf{x}$.\n","\n","So, if $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}= \\begin{pmatrix}\\mathbf{a}_1^\\top\\\\\\mathbf{a}_2^\\top\\\\\\vdots\\\\\\mathbf{a}_m^\\top\\end{pmatrix}$, where $\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_m$ are column vectors, and $\\mathbf{x}\\in \\mathbb{R}^{n} = \\begin{pmatrix}\\mathbf{x}_1\\\\\\mathbf{x}_2\\\\\\vdots\\\\\\mathbf{x}_n\\end{pmatrix}$ is a column vector, then $\\mathbf{Ax}\\in \\mathbb{R}^{m} = \\begin{pmatrix}\\mathbf{a}_1^\\top\\mathbf{x}\\\\\\mathbf{a}_2^\\top\\mathbf{x}\\\\\\vdots\\\\\\mathbf{a}_m^\\top\\mathbf{x}\\end{pmatrix}$\n","\n","A Matrix-Matrix product is the above idea extended to additional dimensions.\n","\n","So, if $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}= \\begin{pmatrix}\\mathbf{a}_1^\\top\\\\\\mathbf{a}_2^\\top\\\\\\vdots\\\\\\mathbf{a}_m^\\top\\end{pmatrix}$, where $\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_m$ are column vectors, and $\\mathbf{B} \\in \\mathbb{R}^{n\\times k}= \\begin{pmatrix}\\mathbf{b}_1 && \\mathbf{b}_2 && \\dots && \\mathbf{b}_k\\end{pmatrix}$, where $\\mathbf{b}_1, \\mathbf{b}_2, \\dots, \\mathbf{b}_k$ are column vectors, then $\\mathbf{AB} \\in \\mathbb{R}^{m\\times k}= \\begin{pmatrix}\\mathbf{a}_1^\\top\\mathbf{b}_1 && \\mathbf{a}_1^\\top\\mathbf{b}_2 &&\\dots && \\mathbf{a}_1^\\top\\mathbf{b}_k \\\\\\mathbf{a}_2^\\top\\mathbf{b}_1 && \\mathbf{a}_2^\\top\\mathbf{b}_2 &&\\dots && \\mathbf{a}_2^\\top\\mathbf{b}_k \\\\\\vdots && \\vdots && \\ddots && \\vdots\\\\\\mathbf{a}_m^\\top\\mathbf{b}_1 && \\mathbf{a}_m^\\top\\mathbf{b}_2 &&\\dots && \\mathbf{a}_m^\\top\\mathbf{b}_k \\\\\\end{pmatrix}$"],"metadata":{"id":"4z82IMz17Zb8"}},{"cell_type":"code","source":["x = torch.tensor([1., 2., 0.])\n","y = torch.ones(3) * 2\n","print(\"x:\", x)\n","print(\"y:\", y)\n","print(\"dot product:\", torch.dot(x, y))\n","print(\"dot product (second way):\", torch.sum(x * y))\n","\n","A = torch.arange(6, dtype = torch.float32).reshape(2,3)\n","print(\"\\nA:\\n\", A)\n","print(\"x:\", x)\n","print(\"matrix-vector product:\\n\", torch.mv(A, x))\n","print(\"matrix-vector product: (second way)\\n\", A@x)\n","\n","A = torch.arange(6, dtype = torch.float32).reshape(2,3)\n","B = A.T\n","print(\"\\nA:\\n\", A)\n","print(\"B:\\n\", B)\n","print(\"matrix-matrix product: \\n\", A@B)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eh0mxVjG49L2","executionInfo":{"status":"ok","timestamp":1680664850351,"user_tz":420,"elapsed":210,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"a84a4be5-1170-481b-a34a-f326f4101c58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([1., 2., 0.])\n","y: tensor([2., 2., 2.])\n","dot product: tensor(6.)\n","dot product (second way): tensor(6.)\n","\n","A:\n"," tensor([[0., 1., 2.],\n","        [3., 4., 5.]])\n","x: tensor([1., 2., 0.])\n","matrix-vector product:\n"," tensor([ 2., 11.])\n","matrix-vector product: (second way)\n"," tensor([ 2., 11.])\n","\n","A:\n"," tensor([[0., 1., 2.],\n","        [3., 4., 5.]])\n","B:\n"," tensor([[0., 3.],\n","        [1., 4.],\n","        [2., 5.]])\n","matrix-matrix product: \n"," tensor([[ 5., 14.],\n","        [14., 50.]])\n"]}]},{"cell_type":"markdown","source":["### Norms\n","\n","$l_p$ norm: $\\|\\mathbf{x}\\|_p=\\left( \\sum_{i=0}^{n} |x_i|^p\\right)^{1/p}$\n","\n","A familiar form of this is when $p=2$, which is called the $l_2$ norm, which denotes the Euclidean length of a vector:  \n","$\\|\\mathbf{x}\\|_2=\\left( \\sum_{i=0}^{n} x_i^2\\right)^{1/2}$\n","\n","Another common one is when $p=1$, which is called the $l_1$ norm, which calculates the Manhattan distance:  \n","$\\|\\mathbf{x}\\|_1= \\sum_{i=0}^{n} |x_i|$"],"metadata":{"id":"kTFneGYYEL-r"}},{"cell_type":"code","source":["g = torch.tensor([3., -4.])\n","print(\"g:\", g)\n","print(\"l2 norm:\", torch.norm(g))\n","print(\"l1 norm:\", torch.abs(g).sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbIlFrPA-DKW","executionInfo":{"status":"ok","timestamp":1680664853015,"user_tz":420,"elapsed":188,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"0022d90c-afdb-4315-9380-5662409eeeb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["g: tensor([ 3., -4.])\n","l2 norm: tensor(5.)\n","l1 norm: tensor(7.)\n"]}]},{"cell_type":"markdown","source":["## Calculus"],"metadata":{"id":"BBQ6ShWaHQe1"}},{"cell_type":"markdown","source":["### Derivatives and Differentiation\n","\n","The derivative of a function $f$ at point $x$ is given by $f'(x) = \\lim_{h \\to 0}\\frac{f(x+h)-f(x)}{h}$. It is the instantaneous rate of change of $f$ with respect to $x$.\n","\n","When $f'(x)$ exists for all $x$ in a given interval $[a, b]$, then $f$ is said to be differentiable on this set.\n","\n"],"metadata":{"id":"7yHfHCVDHgKc"}},{"cell_type":"code","source":["def f(x):\n","    return 3 * x ** 2 - 4 * x\n","\n","def f_prime(x):\n","    return 6 * x - 4\n","\n","print(\"f'(1):\", f_prime(1))\n","\n","print(\"\\nFor x = 1, as h approaches 0:\")\n","for h in 10.0**torch.arange(-1, -6, -1):\n","    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foD5Tr3-HS7s","executionInfo":{"status":"ok","timestamp":1680664854713,"user_tz":420,"elapsed":174,"user":{"displayName":"Yash Shah","userId":"15760933725461182782"}},"outputId":"d7972a12-0576-4415-bed2-bc4207b099c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f'(1): 2\n","\n","For x = 1, as h approaches 0:\n","h=0.10000, numerical limit=2.30000\n","h=0.01000, numerical limit=2.02999\n","h=0.00100, numerical limit=2.00295\n","h=0.00010, numerical limit=2.00033\n","h=0.00001, numerical limit=2.00272\n"]}]},{"cell_type":"markdown","source":["### Common Derivatives\n","\n","- $\\frac{d}{dx}C = 0$ for any constant C\n","- $\\frac{d}{dx}x^n = nx^{n-1}$ for $n \\ne 0$\n","- $\\frac{d}{dx}e^x = e^x$\n","- $\\frac{d}{dx}\\log(x) = \\frac{1}{x}$"],"metadata":{"id":"VaO8mknoZzBH"}},{"cell_type":"markdown","source":["### Properties of Derivatives\n","\n","- Constant Multiple Rule  \n","$\\frac{d}{dx}[C f(x)] = C\\frac{d}{dx}f(x)$ for any constant C  \n","- Sum Rule  \n","$\\frac{d}{dx}[f(x) + g(x)] = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$  \n","- Product Rule  \n","$\\frac{d}{dx}[f(x)g(x)] = f(x)\\frac{d}{dx}g(x) + g(x)\\frac{d}{dx}f(x)$\n","- Quotient Rule  \n","$\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x)\\frac{d}{dx}f(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2}$"],"metadata":{"id":"9L6EleJhaVl3"}},{"cell_type":"markdown","source":["Quick demo:\n","\n","If $f(x) = x\\log(x)$, then  \n","$f'(x) = x \\left(\\frac{d}{dx} \\log(x)\\right) + \\left(\\frac{d}{dx}x\\right) \\log(x) = x (1/x) + 1.\\log(x) = 1 + \\log(x)$"],"metadata":{"id":"jETWmspXKgn4"}},{"cell_type":"markdown","source":["### Partial Derivatives and Gradients\n","\n","Extending the above to multivariate functions,\n","\n","$\\frac{\\partial f}{\\partial x_i} = \\partial_{x_i}f = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_{i-1}, x_i + h, x_{i+1}, \\dots, x_n) - f(x_1, \\dots, x_i, \\dots, x_n)}{h}$\n","\n","Gradient of function $f$ with respect to $x$ is a vector of $n$ partial derivatives:\n","\n","$\\nabla_x f(x) = [\\partial_{x_1}f, \\partial_{x_2}f, \\dots, \\partial_{x_n}f]^\\top$\n"],"metadata":{"id":"SyGHdh03LfSs"}},{"cell_type":"markdown","source":["### Chain Rule\n","\n","To help with calculating the gradients of deeply nested functions (functions of functions of ...), we use the chain rule.\n","\n","In the univariate case, if $y = f(u)$ and $u = g(x)$, then\n","$\\frac{dy}{dx} = \\frac{df}{du}\\frac{du}{dx}$\n","\n","For example,  \n","$\\frac{d\\log(x^2)}{dx} = \\frac{d\\log(x^2)}{dx^2}\\frac{dx^2}{dx} = \\frac{1}{x^2}\\cdot 2x = \\frac{2}{x}$\n","\n","In the multivariate case, if $y = f(u_1, u_2, \\dots, u_m)$ and $u_i = g_i(x_1, x_2, \\dots, x_n)$, then  \n","$\\frac{dy}{dx_i} = \\frac{df}{du_1}\\frac{du_1}{dx_i} + \\frac{df}{du_2}\\frac{du_2}{dx_i} + \\dots + \\frac{df}{du_m}\\frac{du_m}{dx_i}$\n","\n","For example,  \n","If $z(x, y) = \\log(x + y)$, where $x(t) = t^2$ and $y(t) = t$, then  \n","$\\frac{dz}{dt} = \\frac{dz}{dx}\\frac{dx}{dt} + \\frac{dz}{dy}\\frac{dy}{dt} = \\frac{1}{x}\\cdot 2t + \\frac{1}{y}\\cdot 1 = \\frac{1}{t^2}\\cdot 2t + \\frac{1}{t} = \\frac{2}{t} + \\frac{1}{t} = \\frac{3}{t}$"],"metadata":{"id":"uDVXBZIZLiD8"}},{"cell_type":"code","source":[],"metadata":{"id":"r_N_q47hsJ89"},"execution_count":null,"outputs":[]}]}