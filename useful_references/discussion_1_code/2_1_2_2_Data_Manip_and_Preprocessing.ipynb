{"cells":[{"cell_type":"markdown","metadata":{"id":"TEPJbmycMZil"},"source":["# 2.1 Data Manipulation\n","\n","(https://d2l.ai/chapter_preliminaries/ndarray.html)\n","\n","Prerequisites: Installing Pytorch\n","- Install using your preferred method (`pip` or `conda` would be easiest) and for your corresponding operating system: https://pytorch.org/\n","- The PyTorch version should not matter too much. \n","- Make sure to install the Cuda version!!"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFBjAN_7MZin","outputId":"1a323dd2-4615-4325-cdcd-66a7cf19b655","executionInfo":{"status":"ok","timestamp":1680982874666,"user_tz":420,"elapsed":5030,"user":{"displayName":"James Zhao","userId":"02681184463155793894"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version: 2.0.0+cu118\n"]}],"source":["import numpy as np, torch\n","print(f\"PyTorch Version: {torch.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"etCo4BjfMZio"},"source":["## Tensor Basics & Getting Started"]},{"cell_type":"markdown","metadata":{"id":"Cz4-6gGSMZip"},"source":["### Creating / Modifying Tensors\n","\n","**Tensors** are pytorch's primary way of store data/matrices/tensors and can store arbitrary dimensional data with several associated datatype. Here are several ways of creating a 1-dimensional Float32 Tensor of the values (1, 2, 3, 4): \n","\n","$$\\begin{bmatrix}1 & 2 & 3 & 4\\end{bmatrix}$$\n","\n","Mathematically, a tensor can be seed as a generalization of a matrix to an arbitrary number of dimensions. The **dimension** of a tensor is somewhat similar to how many axes the tensor has. \n","- A matrix is a 2-dimensional(2d) tensor\n","- An array is typically a 1-dimensional(1d) tensor\n","- A singular value is a 0-dimensional(0d) tensor\n","  \n","https://pytorch.org/docs/1.3.1/tensors.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0yL8u6iMZip","outputId":"be038570-88ea-4ec0-a8aa-9760bb3a6b91"},"outputs":[{"name":"stdout","output_type":"stream","text":["a=tensor([1., 2., 3., 4.]) a.type()='torch.FloatTensor' b=tensor([1., 2., 3., 4.]) b.type()='torch.FloatTensor'\n","c=tensor([1., 2., 3., 4.]) c.type()='torch.FloatTensor' d=tensor([1., 2., 3., 4.]) d.type()='torch.FloatTensor'\n","e=tensor([1., 2., 3., 4.]) e.type()='torch.FloatTensor' f=tensor([1., 2., 3., 4.]) f.type()='torch.FloatTensor'\n","g=tensor([1., 2., 3., 4.]) g.type()='torch.FloatTensor'\n"]}],"source":["my_np_array = np.array((1, 2, 3, 4), dtype=np.float32)\n","\n","# Construct from tuples\n","a = torch.tensor((1, 2, 3, 4), dtype=torch.float32)\n","b = torch.Tensor((1, 2, 3, 4))\n","print(f\"{a=} {a.type()=} {b=} {b.type()=}\")\n","\n","# Construct from lists\n","c = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","d = torch.Tensor([1, 2, 3, 4])\n","print(f\"{c=} {c.type()=} {d=} {d.type()=}\")\n","\n","# Construct from np array\n","e = torch.tensor(my_np_array, dtype=torch.float32)\n","f = torch.Tensor(my_np_array)\n","print(f\"{e=} {e.type()=} {f=} {f.type()=}\")\n","\n","# Other ways to construct tensors\n","g = torch.arange(1, 5, dtype=torch.float32) # https://pytorch.org/docs/stable/generated/torch.arange.html\n","print(f\"{g=} {g.type()=}\")"]},{"cell_type":"markdown","metadata":{"id":"bciVD1NLMZiq"},"source":["### Common PyTorch Operations"]},{"cell_type":"markdown","metadata":{"id":"xy7KeM-RMZiq"},"source":["There are many PyTorch operations that you can perform on tensors. Some of the common ones include:\n","- `x.numel()`: getting the number of elements in a tensor \n","- `x.shape` or `x.size()`: gets the size of the tensor (size of each dimension as a tuple)\n","- `x.T`: matrix transpose (for 2d tensor)\n","- `x.reshape()`: reshapes a tensor to a desired dimension (if possible)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s00PMctDMZiq","outputId":"da76cd62-5448-4592-a311-327fd9feea7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["x=tensor([[ 1.,  2.,  3.,  4.],\n","        [ 5.,  6.,  7.,  8.],\n","        [ 9., 10., 11., 12.]])\n","x.numel()=12\n","x.shape=torch.Size([3, 4]) x.size()=torch.Size([3, 4])\n","x.T=tensor([[ 1.,  5.,  9.],\n","        [ 2.,  6., 10.],\n","        [ 3.,  7., 11.],\n","        [ 4.,  8., 12.]])\n","--------------------\n","x.reshape((12, 1))=tensor([[ 1.],\n","        [ 2.],\n","        [ 3.],\n","        [ 4.],\n","        [ 5.],\n","        [ 6.],\n","        [ 7.],\n","        [ 8.],\n","        [ 9.],\n","        [10.],\n","        [11.],\n","        [12.]])\n","x.reshape((1, 12))=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.]])\n","tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n","tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n","tensor([[ 1.,  2.,  3.],\n","        [ 4.,  5.,  6.],\n","        [ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n"]}],"source":["# Construct a 3x4 matrix of the values [1,12]\n","x = torch.Tensor([\n","  [1, 2, 3, 4],\n","  [5, 6, 7, 8],\n","  [9, 10, 11, 12]\n","])\n","\n","print(f\"{x=}\")\n","# Numel = [Num]ber of [El]ements. Returns number of elements in the tensor. 3 rows * 4 columns = 9 elements\n","print(f\"{x.numel()=}\") \n","# Returns the size of the matrix/tensor. For 2x2 case, returns (rows, columns) as a 'torch.Size', which is similar to a tuple\n","print(f\"{x.shape=} {x.size()=}\") \n","# Perform a matrix transpose (A^T)\n","print(f\"{x.T=}\")\n","\n","print('-'*20)\n","\n","# Reshaping (https://pytorch.org/docs/stable/generated/torch.reshape.html)\n","print(f\"{x.reshape((12, 1))=}\") # 12 rows, 1 column\n","print(f\"{x.reshape((1, 12))=}\") # 1 row, 12 columns\n","print(f\"{x.reshape((12))}\") # 12 items in 1d tensor\n","print(f\"{x.reshape((-1))}\") # -1 causes pytorch to infer the size of that dimension\n","print(f\"{x.reshape((-1, 3))}\") # -1 causes pytorch to infer the size of that dimension\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_iZyfItMZir","outputId":"1160e1f7-e23b-470a-e6e2-d837719f680d","executionInfo":{"status":"ok","timestamp":1680898572228,"user_tz":420,"elapsed":31,"user":{"displayName":"James Zhao","userId":"02681184463155793894"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["zeros_tensor=tensor([[0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.]]) \n","  shape=torch.Size([5, 7])\n","ones_tensor=tensor([[1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1.]]) \n","  shape=torch.Size([3, 7])\n"]}],"source":["zeros_tensor = torch.zeros((5, 7)) # creates a tensor of zeroes of an arbitrary shape\n","ones_tensor = torch.ones((3, 7)) # \"\"\" \"\"\" but for ones\n","\n","print(f\"{zeros_tensor=} \\n  shape={zeros_tensor.size()}\")\n","print(f\"{ones_tensor=} \\n  shape={ones_tensor.size()}\")"]},{"cell_type":"markdown","metadata":{"id":"su19Qwb6MZir"},"source":["# Indexing and Slicing"]},{"cell_type":"markdown","metadata":{"id":"k5jnDWK5MZis"},"source":["Like with Python Lists, you can use Python's list indexing and slicing to extract a range of elements. \n","- https://stackoverflow.com/a/509295\n","- https://www.geeksforgeeks.org/python-list-slicing/#\n","\n","Each dimension can be sliced with the following format: `tensor[start:stop:stepsize]` or `tensor[start:stop]`, where the value of `stop` is not included, and both `start` and `stop` are zero-based indexes.\n","- If one of the values `start`, `stop`, `stepsize` is excluded, a default is automatically used.\n","  - Think of this as a for loop! <br>\n","    ```for (int i = start; i < end; i += stepsize) extract(i)```\n","  - If a value is not explicitly given, start's default is `0`, stop's default is `n` (number of elements along a dimension), stepsize is `1`\n","- You can use negative indexes to signify values from the end of the list (-1 -> last item (n - 1), -2 -> second to last item (n - 2), etc...)\n","- multiple dimensions can be sliced by seperating using a comma(`,`)\n","\n","#### Examples:\n","$x = \\begin{bmatrix}1 & 2 & 3 & 4 \\\\ 5 & 6 & 7 & 8 \\\\ 9 & 10 & 11 & 12\\end{bmatrix}$\n","\n","$y = \\begin{bmatrix}1 & 2 & 3 & 4 \\end{bmatrix}$\n","\n","- `x[-1]`: Extracts the **last row** of the 2d matrix as a 1d-tensor (`[9, 10, 11, 12]`)\n","- `y[-1]`: Extracts the last element of the 1d-array as a 0-dimensional single value(`4`)\n","- `x[:,0]`: Extracts all elements along the first dimension (due to the **singular** colon), and extracts the first value along the second dimension. \n","  - Equivalent to extracting the first column as a 1-d array. (`[1, 5, 9]`)\n","- `y[::]` or `y[:]`: Extracts all elements of the tensor (essentially identiy)\n","- `x[2, 3]` Extracts the item in row-index 2 and column-index 3 of x as a 0d-tensor. (`12`)\n","  - The dimension is reduced by two since both dimension are sliced by a single value. \n","- `y[1::2]`: Extracts all odd-index items of `y` (`[2, 4]`)\n","\n","#### Exercise: What would each output and what is the output dimension?\n","- `y[1:]`?\n","- `x[2,:]`?\n","- `x[::2,::2]`?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJDonb9nMZis","outputId":"5179ed55-c994-49e5-c198-b909e9824de1"},"outputs":[{"name":"stdout","output_type":"stream","text":["x[-1]=tensor([ 9., 10., 11., 12.]) output_dimension=1\n","y[-1]=tensor(4.) output_dimension=0\n","x[:,0]=tensor([1., 5., 9.]) output_dimension=1\n","y[::]=tensor([1., 2., 3., 4.]) tensor([1., 2., 3., 4.]) output_dimension=1\n","x[2,3]=tensor(12.) output_dimension=0\n","y[1::2]=tensor([2., 4.]) output_dimension=1\n"]}],"source":["x = torch.Tensor([\n","    [1, 2, 3, 4],\n","    [5, 6, 7, 8],\n","    [9, 10, 11, 12]\n","])\n","y = torch.Tensor([1, 2, 3, 4])\n","\n","print(f\"{x[-1]=} output_dimension={len(x[-1].size())}\")\n","print(f\"{y[-1]=} output_dimension={len(y[-1].size())}\")\n","print(f\"{x[:,0]=} output_dimension={len(x[:,0].size())}\")\n","print(f\"{y[::]=} {y[:]} output_dimension={len(y[::].size())}\")\n","print(f\"{x[2,3]=} output_dimension={len(x[2,3].size())}\")\n","print(f\"{y[1::2]=} output_dimension={len(y[1::2].size())}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKPwa8HOMZis","outputId":"bd49d459-043a-4d2e-e38d-bab6bb82ac72"},"outputs":[{"name":"stdout","output_type":"stream","text":["y[1:]=tensor([2., 3., 4.]) output_dimension=1\n","x[2,2:]=tensor([11., 12.]) output_dimension=1\n","x[::2,::2]=tensor([[ 1.,  3.],\n","        [ 9., 11.]]) output_dimension=2\n"]}],"source":["# Exercises\n","print(f\"{y[1:]=} output_dimension={len(y[1:].size())}\")\n","print(f\"{x[2,2:]=} output_dimension={len(x[2,2:].size())}\")\n","print(f\"{x[::2,::2]=} output_dimension={len(x[::2,::2].size())}\")"]},{"cell_type":"markdown","metadata":{"id":"SL3b9JE3MZis"},"source":["## PyTorch Operations\n","\n","Many python operations exist in pytorch as well:\n","- `+`,`-`,`*`,`/`,`**`(exponential) are all **ELEMENT-WISE** (note that `*` is NOT matrix multiplication)\n","  - Each can be done with (matrix,matrix), (scalar,matrix), (matrix,scalar)\n","- `@` is matrix multiplication\n","- `==` (double equals): Will perform an element-wise equality check, returning a binary matrix of `True/False`\n","\n","Other common operations:\n","- `torch.exp(a)`: Element wise $e^x$\n","- `torch.cat((a, b), dim)`: Concatenate two tensors along the specified dimension\n","  - E.x. in a 2d tensor, 0 = row-wise, 1 = column-wise\n","- `torch.sum(a, dim, keepdims=False)`: Either sums all values in a tensor (if dim is not specified), or sum along the dimension (if specified)\n","  - E.x. dim=0 does a row-wise sum\n","  - Normally, summing along a dimension removes that dimension. `keepdims=True` will keep that dimension as a length-1 dimension\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TxFhsbEMZit","outputId":"b287c59a-5486-4019-a9e7-d8a6f0587f8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["x+y=tensor([[ 2.,  3.,  4.],\n","        [ 5.,  6.,  7.],\n","        [ 8.,  9., 10.]])\n","x*y=tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","--------------------\n","x@y=tensor([[12., 15., 18.],\n","        [12., 15., 18.],\n","        [12., 15., 18.]])\n","torch.exp(x)=tensor([[2.7183, 2.7183, 2.7183],\n","        [2.7183, 2.7183, 2.7183],\n","        [2.7183, 2.7183, 2.7183]])\n","Row-Wise:\n","  torch.cat((x, y), dim=0)=tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","Column-Wise:\n","  torch.cat((x, y), dim=1)=tensor([[1., 1., 1., 1., 2., 3.],\n","        [1., 1., 1., 4., 5., 6.],\n","        [1., 1., 1., 7., 8., 9.]])\n","torch.sum(y, dim=0)=tensor([12., 15., 18.])\n"]}],"source":["x = torch.Tensor([\n","    [1, 1, 1],\n","    [1, 1, 1],\n","    [1, 1, 1]\n","])\n","y = torch.Tensor([\n","    [1, 2, 3],\n","    [4, 5, 6],\n","    [7, 8, 9]\n","])\n","\n","print(f\"{x+y=}\")\n","print(f\"{x*y=}\")\n","\n","print('-'*20)\n","\n","print(f\"{x@y=}\")\n","print(f\"{torch.exp(x)=}\")\n","print(f\"Row-Wise:\\n  {torch.cat((x, y), dim=0)=}\")\n","print(f\"Column-Wise:\\n  {torch.cat((x, y), dim=1)=}\")\n","print(f\"{torch.sum(y, dim=0)=}\")"]},{"cell_type":"markdown","metadata":{"id":"x-V05u98MZit"},"source":["## Tensor Broadcasting\n","\n","When performing an operation (such as addition) between two tensors of **differing** shapes, the operaton can still sometimes be performed by **broadcasting** one of the tensors.\n","- When one of the tensors has a dimension with a size of 1 (e.x. 1 row or 1 column), the tensor is duplicated to match the corresponding size of the other tensor. \n","\n","Example: \n","$x = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}, y = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$\n","- x has a shape of `(2, 2)`, and y has a shape of `(2, 1)`\n","- The second dimension of y has a size of 1 (1 column), so PyTorch copies `y` along the column-wise axis until it has a size of `(2,2)`. Essentially, $y' = \\begin{bmatrix}1 & 1 \\\\ 2 & 2\\end{bmatrix}$\n","- After broadcasting, both tensors have size `(2,2)`. Then, a standard element-wise operation is performed, resulting in $x+y=\\begin{bmatrix}2 & 3 \\\\ 5 & 6\\end{bmatrix}$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWWz-o6GMZit","outputId":"7e3fd08a-81d0-4a48-dae7-53ec9149666d"},"outputs":[{"name":"stdout","output_type":"stream","text":["x+y=tensor([[2., 3.],\n","        [5., 6.]])\n"]}],"source":["x = torch.Tensor([\n","    [1, 2],\n","    [3, 4]\n","])\n","y = torch.Tensor([\n","    [1],\n","    [2]\n","])\n","print(f\"{x+y=}\")"]},{"cell_type":"markdown","metadata":{"id":"2-ujd2n0MZit"},"source":["## Conversions To and Between Other Types\n","\n","Tensors can be easily converted to Numpy's `ndarray`:\n","- 1-item tensors can be converted to python primitives using `x.item()` or Python's built in casts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7s8vHE_MZit","outputId":"f7576962-afae-4dfc-b66a-10942f7340aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'> x=array([[1, 2],\n","       [3, 4]])\n","<class 'torch.Tensor'> b=tensor([[1, 2],\n","        [3, 4]], dtype=torch.int32)\n"]}],"source":["x = np.array([\n","    [1, 2],\n","    [3, 4]\n","])\n","\n","b = torch.from_numpy(x)\n","print(f\"{type(x)} {x=}\")\n","print(f\"{type(b)} {b=}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKGCePIqMZiu","outputId":"d7c49b91-7fda-412c-97ad-add73ea0fb6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'> c=tensor([1.])\n","<class 'float'> c.item()=1.0\n","<class 'float'> float(c)=1.0\n"]}],"source":["c = torch.Tensor([1])\n","print(f\"{type(c)} {c=}\")\n","print(f\"{type(c.item())} {c.item()=}\")\n","print(f\"{type(float(c))} {float(c)=}\")"]},{"cell_type":"markdown","metadata":{"id":"men6vJa0MZiu"},"source":["# 2.2 Pre Processing"]},{"cell_type":"markdown","metadata":{"id":"Y4vjJoKdMZiu"},"source":["This tutorial will follow 2.2 of Dive Into Deep Learning, and use the Kaggle Competiton dataset as an example\n","\n","https://d2l.ai/chapter_preliminaries/pandas.html\n","\n","https://www.kaggle.com/competitions/ucsd-cse-151b-class-competition/data\n","\n","(If you are running this in Colab, you will likely have to mount your google drive and upload the dataset!)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BGax6QkbMZiu","executionInfo":{"status":"ok","timestamp":1680982876441,"user_tz":420,"elapsed":470,"user":{"displayName":"James Zhao","userId":"02681184463155793894"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"id":"xDpIdoHHMZiu","outputId":"509d0f06-ea72-489c-fadd-0ba4639aaa65"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== Raw Data ===\n","\"TRIP_ID\",\"CALL_TYPE\",\"ORIGIN_CALL\",\"ORIGIN_STAND\",\"TAXI_ID\",\"TIMESTAMP\",\"DAY_TYPE\",\"MISSING_DATA\",\"POLYLINE\"\n","\"1372636858620000589\",\"C\",\"\",\"\",\"20000589\",\"1372636858\",\"A\",\"False\",\"[[-8.618643,41.141412],[-8.618499,41.141376],[-8.620326,41.14251],[-8.622153,41.143815],[-8.623953,41.144373],[-8.62668,41.144778],[-8.627373,41.144697],[-8.630226,41.14521],[-8.632746,41.14692],[-8.631738,41.148225],[-8.629938,41.150385],[-8.62911,41.151213],[-8.629128,41.15124],[-8.628786,41.152203],[-8.628687,41.152374],[-8.628759,41.152518],[-8.630838,41.15268],[-8.632323,41.153022],[-8.631144,41.154489],[-8.630829,41.154507],[-8.630829,41.154516],[-8.630829,41.154498],[-8.630838,41.154489]]\"\n","\"1372637303620000596\",\"B\",\"\",\"7\",\"20000596\",\"1372637303\",\"A\",\"False\",\"[[-8.639847,41.159826],[-8.640351,41.159871],[-8.642196,41.160114],[-8.644455,41.160492],[-8.646921,41.160951],[-8.649999,41.161491],[-8.653167,41.162031],[-8.656434,41.16258],[-8.660178,41.163192],[-8.663112,41.163687],[-8.666235,41.1642],[-8.669169,41.164704],[-8.670852,41.165136],[-8.670942,41.166576],[-8.66961,41.167962],[-8.668098,41.168988],[-8.66664,41.170005],[-8.665767,41.170635],[-8.66574,41.170671]]\"\n","\"1372636951620000320\",\"C\",\"\",\"\",\"20000320\",\"1372636951\",\"A\",\"False\",\"[[-8.612964,41.140359],[-8.613378,41.14035],[-8.614215,41.140278],[-8.614773,41.140368],[-8.615907,41.140449],[-8.616609,41.140602],[-8.618472,41.141412],[-8.620623,41.142789],[-8.622558,41.144094],[-8.62506,41.144805],[-8.627436,41.144733],[-8.630082,41.145174],[-8.6319,41.146461],[-8.632584,41.147316],[-8.631252,41.148774],[-8.629713,41.150628],[-8.628804,41.152077],[-8.628579,41.152464],[-8.62875,41.152662],[-8.630424,41.15277],[-8.632683,41.152779],[-8.635131,41.152563],[-8.637705,41.153013],[-8.64036,41.15358],[-8.642205,41.154021],[-8.644068,41.154507],[-8.646453,41.154336],[-8.648613,41.1543],[-8.649504,41.154336],[-8.649837,41.154354],[-8.649837,41.1543],[-8.649882,41.154282],[-8.649936,41.1543],[-8.6499,41.154264],[-8.599383,41.141736],[-8.59653,41.140566],[-8.65008,41.154291],[-8.650395,41.153814],[-8.650377,41.153832],[-8.650359,41.153787],[-8.649891,41.153166],[-8.649369,41.152572],[-8.649198,41.152374],[-8.649711,41.151213],[-8.649117,41.150466],[-8.649117,41.149062],[-8.648613,41.148261],[-8.648424,41.148225],[-8.647587,41.148405],[-8.64594,41.148414],[-8.643861,41.148135],[-8.642763,41.148027],[-8.640918,41.14836],[-8.637759,41.148351],[-8.635338,41.147964],[-8.633277,41.147172],[-8.631513,41.146146],[-8.629776,41.14503],[-8.627814,41.144643],[-8.625996,41.144769],[-8.624088,41.144463],[-8.621325,41.143401],[-8.619444,41.141961],[-8.617365,41.140863],[-8.61597,41.14053]]\"\n","\"1372636854620000520\",\"C\",\"\",\"\",\"20000520\",\"1372636854\",\"A\",\"False\",\"[[-8.574678,41.151951],[-8.574705,41.151942],[-8.574696,41.151933],[-8.57466,41.15196],[-8.574723,41.151933],[-8.574714,41.151924],[-8.574714,41.151924],[-8.575164,41.150934],[-8.577135,41.150232],[-8.57853,41.148639],[-8.579745,41.147316],[-8.579358,41.146173],[-8.580744,41.14503],[-8.582904,41.14512],[-8.58438,41.146479],[-8.610849,41.145876],[-8.610012,41.146479],[-8.609058,41.146866],[-8.608968,41.147055],[-8.586027,41.148702],[-8.587197,41.149224],[-8.588205,41.148963],[-8.588835,41.147604],[-8.590176,41.147082],[-8.592543,41.146614],[-8.594721,41.146245],[-8.596737,41.146317],[-8.59869,41.146119],[-8.598816,41.146101],[-8.600193,41.146155],[-8.601057,41.146101],[-8.602344,41.14575],[-8.602785,41.145705],[-8.60328,41.145597],[-8.604045,41.145417],[-8.604657,41.144256],[-8.605854,41.142555],[-8.607951,41.142753],[-8.607978,41.142825],[-8.607996,41.142879],[-8.607987,41.142888],[-8.608005,41.142915],[-8.607996,41.142915]]\"\n","\n","=== Pandas DataFrame ===\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TRIP_ID</th>\n","      <th>CALL_TYPE</th>\n","      <th>ORIGIN_CALL</th>\n","      <th>ORIGIN_STAND</th>\n","      <th>TAXI_ID</th>\n","      <th>TIMESTAMP</th>\n","      <th>DAY_TYPE</th>\n","      <th>MISSING_DATA</th>\n","      <th>POLYLINE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1372636858620000589</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000589</td>\n","      <td>1372636858</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1372637303620000596</td>\n","      <td>B</td>\n","      <td>NaN</td>\n","      <td>7.0</td>\n","      <td>20000596</td>\n","      <td>1372637303</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1372636951620000320</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000320</td>\n","      <td>1372636951</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1372636854620000520</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000520</td>\n","      <td>1372636854</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1372637091620000337</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000337</td>\n","      <td>1372637091</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n","0  1372636858620000589         C          NaN           NaN  20000589   \n","1  1372637303620000596         B          NaN           7.0  20000596   \n","2  1372636951620000320         C          NaN           NaN  20000320   \n","3  1372636854620000520         C          NaN           NaN  20000520   \n","4  1372637091620000337         C          NaN           NaN  20000337   \n","\n","    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n","0  1372636858        A         False   \n","1  1372637303        A         False   \n","2  1372636951        A         False   \n","3  1372636854        A         False   \n","4  1372637091        A         False   \n","\n","                                            POLYLINE  \n","0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n","1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n","2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n","3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n","4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["print(\"=== Raw Data ===\")\n","with open(\"train.csv\", \"r\") as f:\n","  raw = f.readlines()[:5]\n","  print(\"\".join(raw))\n","print(\"=== Pandas DataFrame ===\")\n","df = pd.read_csv(\"train.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"cTvtynUCMZiu"},"source":["## Preprocessing Example: Obtaining number of latitude/longitude coordinates\n","\n","From the kaggle competition spec:\n","\n","```\n","The travel time of the trip (the prediction target of this project) is defined \n","as the (number of points-1) x 15 seconds. For example, a trip with 101 data \n","points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have \n","missing data points in POLYLINE, indicated by MISSING_DATA column, and it is \n","part of the challenge how you utilize this knowledge.\n","```\n","\n","Thus, having the number of points in a polyline can be a valuable feature. One naive way is to count the number of beginning square brackets (ignoring missing data for this example):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"340WwPLVMZiu","outputId":"1776badd-acdc-4683-a920-437adb38abe8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TRIP_ID</th>\n","      <th>CALL_TYPE</th>\n","      <th>ORIGIN_CALL</th>\n","      <th>ORIGIN_STAND</th>\n","      <th>TAXI_ID</th>\n","      <th>TIMESTAMP</th>\n","      <th>DAY_TYPE</th>\n","      <th>MISSING_DATA</th>\n","      <th>POLYLINE</th>\n","      <th>POLYLINE_CT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1372636858620000589</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000589</td>\n","      <td>1372636858</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1372637303620000596</td>\n","      <td>B</td>\n","      <td>NaN</td>\n","      <td>7.0</td>\n","      <td>20000596</td>\n","      <td>1372637303</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1372636951620000320</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000320</td>\n","      <td>1372636951</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n","      <td>65</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1372636854620000520</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000520</td>\n","      <td>1372636854</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1372637091620000337</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>20000337</td>\n","      <td>1372637091</td>\n","      <td>A</td>\n","      <td>False</td>\n","      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n","      <td>29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n","0  1372636858620000589         C          NaN           NaN  20000589   \n","1  1372637303620000596         B          NaN           7.0  20000596   \n","2  1372636951620000320         C          NaN           NaN  20000320   \n","3  1372636854620000520         C          NaN           NaN  20000520   \n","4  1372637091620000337         C          NaN           NaN  20000337   \n","\n","    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n","0  1372636858        A         False   \n","1  1372637303        A         False   \n","2  1372636951        A         False   \n","3  1372636854        A         False   \n","4  1372637091        A         False   \n","\n","                                            POLYLINE  POLYLINE_CT  \n","0  [[-8.618643,41.141412],[-8.618499,41.141376],[...           23  \n","1  [[-8.639847,41.159826],[-8.640351,41.159871],[...           19  \n","2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...           65  \n","3  [[-8.574678,41.151951],[-8.574705,41.151942],[...           43  \n","4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...           29  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# .apply() repeats a lambda operation value-wise over the POLYLINE column\n","df[\"POLYLINE_CT\"] = df[\"POLYLINE\"].apply(lambda x : max(x.count(\"[\") - 1, 0))\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"aWtcO01eMZiv"},"source":["## Converting Data to Tensors\n","\n","In order to use the PyTorch Framework, dataframes must be converted to (rather large) tensors. As a toy example, let us say that we wanted to have our features be the `Timestamp` and we wanted to predict `POLYLINE_CT`.\n","\n","Notice that we use slicing (treating the dataframe as a large 2d tensor) to extract the desired elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U99IVNRrMZiv","outputId":"18ca73a5-cb61-4958-b7a2-df3ec3419f30"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TIMESTAMP</th>\n","      <th>POLYLINE_CT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1372636858</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1372637303</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1372636951</td>\n","      <td>65</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1372636854</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1372637091</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1710665</th>\n","      <td>1404171463</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>1710666</th>\n","      <td>1404171367</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>1710667</th>\n","      <td>1388745716</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1710668</th>\n","      <td>1404141826</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>1710669</th>\n","      <td>1404157147</td>\n","      <td>27</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1710670 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["          TIMESTAMP  POLYLINE_CT\n","0        1372636858           23\n","1        1372637303           19\n","2        1372636951           65\n","3        1372636854           43\n","4        1372637091           29\n","...             ...          ...\n","1710665  1404171463           32\n","1710666  1404171367           30\n","1710667  1388745716            0\n","1710668  1404141826           62\n","1710669  1404157147           27\n","\n","[1710670 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tensor([1372636858, 1372637303, 1372636951,  ..., 1388745716, 1404141826,\n","        1404157147])\n","tensor([23, 19, 65,  ...,  0, 62, 27])\n"]}],"source":["display(df.iloc[:,[5, 9]])\n","X = torch.tensor(df.iloc[:,5])\n","y = torch.tensor(df.iloc[:,9])\n","print(X)\n","print(y)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"mlenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}